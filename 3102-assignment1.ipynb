{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e6ad16b",
   "metadata": {},
   "source": [
    "Install HuggingSpace transformers package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bec31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install SentencePiece\n",
    "import cProfile\n",
    "import time\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import OpenAIGPTTokenizer\n",
    "from transformers import XLMRobertaTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ee926a",
   "metadata": {},
   "source": [
    "Specify the tokenizer models to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c69031f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_bert = AutoTokenizer.from_pretrained(\"bert-base-uncased\") # BERT is using wordpiece tokenization method\n",
    "tokenizer_openai = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\") # openai-gpt is using BPE tokenization.\n",
    "tokenizer_xlmroberta = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\") # XLM-RoBERTa is using SentencePiece tokenization and supports Unicode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183543ae",
   "metadata": {},
   "source": [
    "Specify the path to the two text file and initialize empty lists to store the lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd9ac02",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'prompts.txt'\n",
    "text_lines = []\n",
    "\n",
    "file_path2 = 'glassdoor-reviews.txt'\n",
    "text_lines2 = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c657b2f7",
   "metadata": {},
   "source": [
    "Open the two text file and read its contents line by line and append them to lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3517a819",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        text_lines.append(line.strip())\n",
    "\n",
    "with open(file_path2, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        text_lines2.append(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f938c621",
   "metadata": {},
   "source": [
    "Tokenizing batched data with the BERT tokenization model and getting the response time with cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c66e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "tokenized_sentences_bert = tokenizer_bert.batch_encode_plus(text_lines, add_special_tokens=True)\n",
    "print(tokenized_sentences_bert[\"input_ids\"])\n",
    "pr.disable()\n",
    "print(\"Printing stats for encoding batched data\")\n",
    "print(\"------------------------------------------\")\n",
    "pr.print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cf7ada",
   "metadata": {},
   "source": [
    "Tokenizing unbatched data with the BERT tokenization model and getting the response time with cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9588b11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "for element in text_lines:\n",
    "    encoding_unbatched = tokenizer_bert.encode_plus(element, add_special_tokens=True, truncation=True)\n",
    "pr.disable()\n",
    "\n",
    "print(\"Printing stats for encoding unbatched data\")\n",
    "print(\"------------------------------------------\")\n",
    "pr.print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdbf4d5",
   "metadata": {},
   "source": [
    "Store the tokenized IDs(Bert tokenization method) in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bcd2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_bert = list(tokenized_sentences_bert[\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e694c57",
   "metadata": {},
   "source": [
    "Detokenizing batched data with the BERT tokenization model and getting the response time with cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76474c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "detokenized_sentences_bert = tokenizer_bert.batch_decode(ids_bert)\n",
    "print(detokenized_sentences_bert)\n",
    "pr.disable()\n",
    "print(\"Printing stats for dencoding batched data\")\n",
    "print(\"------------------------------------------\")\n",
    "pr.print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5e5b02",
   "metadata": {},
   "source": [
    "Detokenizing unbatched data with the BERT tokenization model and getting the response time with cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191d4fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "for element in ids_bert:\n",
    "    detokenized_sentence_bert = tokenizer_bert.decode(element, skip_special_tokens=True)\n",
    "    print(detokenized_sentence_bert)\n",
    "pr.disable()\n",
    "print(\"Printing stats for decoding unbatched data\")\n",
    "print(\"------------------------------------------\")\n",
    "pr.print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb2d685",
   "metadata": {},
   "source": [
    "Tokenizing batched data with the openai-gpt tokenization model and getting the response time with cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf9588d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "tokenized_sentences_openai = tokenizer_openai.batch_encode_plus(text_lines, add_special_tokens=True)\n",
    "#print(tokenized_sentences_openai[\"input_ids\"])\n",
    "pr.disable()\n",
    "print(\"Printing stats for encoding batched data\")\n",
    "print(\"------------------------------------------\")\n",
    "pr.print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c454b36",
   "metadata": {},
   "source": [
    "Store the tokenized IDs(openai tokenization method) in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e963f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_openai = list(tokenized_sentences_openai[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b65f177",
   "metadata": {},
   "source": [
    "Tokenizing unbatched data with the openai-gpt tokenization model and getting the response time with cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d65a4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "for element in text_lines:\n",
    "    encoding_unbatched = tokenizer_openai.encode_plus(element, add_special_tokens=True, truncation=True)\n",
    "    #print(encoding_unbatched)\n",
    "pr.disable()\n",
    "\n",
    "print(\"Printing stats for encoding unbatched data\")\n",
    "print(\"------------------------------------------\")\n",
    "pr.print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14cb729",
   "metadata": {},
   "source": [
    "Detokenizing batched data with the openai tokenization model and getting the response time with cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e349ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "detokenized_sentences_openai = tokenizer_openai.batch_decode(ids_openai)\n",
    "print(detokenized_sentences_openai)\n",
    "pr.disable()\n",
    "print(\"Printing stats for dencoding batched data\")\n",
    "print(\"------------------------------------------\")\n",
    "pr.print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39857d7",
   "metadata": {},
   "source": [
    "Detokenizing unbatched data with the openai tokenization model and getting the response time with cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa645a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "for element in ids_openai:\n",
    "    detokenized_sentence_openai = tokenizer_openai.decode(element, skip_special_tokens=True)\n",
    "    print(detokenized_sentence_openai)\n",
    "pr.disable()\n",
    "print(\"Printing stats for decoding unbatched data\")\n",
    "print(\"------------------------------------------\")\n",
    "pr.print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8948fe",
   "metadata": {},
   "source": [
    "Tokenizing batched data with the XLM-RoBERTa tokenization model and getting the response time with cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441a4139",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "tokenized_sentences_xlmroberta = tokenizer_xlmroberta.batch_encode_plus(text_lines, add_special_tokens=True)\n",
    "print(tokenized_sentences_xlmroberta[\"input_ids\"])\n",
    "pr.disable()\n",
    "print(\"Printing stats for encoding batched data\")\n",
    "print(\"------------------------------------------\")\n",
    "pr.print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f644dadd",
   "metadata": {},
   "source": [
    "Tokenizing unbatched data with the XLM-RoBERTa tokenization model and getting the response time with cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d0b06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "for element in text_lines:\n",
    "    encoding_unbatched = tokenizer_xlmroberta.encode_plus(element, add_special_tokens=True, truncation=True)\n",
    "    #print(encoding_unbatched)\n",
    "pr.disable()\n",
    "\n",
    "print(\"Printing stats for encoding unbatched data\")\n",
    "print(\"------------------------------------------\")\n",
    "pr.print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e2ed30",
   "metadata": {},
   "source": [
    "Store the tokenized IDs(XLM-RoBERTa tokenization method) in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bad818b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_xlmroberta = list(tokenized_sentences_xlmroberta[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5393e31c",
   "metadata": {},
   "source": [
    "Detokenizing batched data with the XLM-RoBERTa tokenization model and getting the response time with cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c425a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "detokenized_sentences_xlmroberta = tokenizer_xlmroberta.batch_decode(ids_xlmroberta)\n",
    "print(detokenized_sentences_xlmroberta)\n",
    "pr.disable()\n",
    "print(\"Printing stats for dencoding batched data\")\n",
    "print(\"------------------------------------------\")\n",
    "pr.print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0694ce3a",
   "metadata": {},
   "source": [
    "Detokenizing unbatched data with the XLM-RoBERTa tokenization model and getting the response time with cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf87d22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "for element in ids_xlmroberta:\n",
    "    detokenized_sentences_xlmroberta = tokenizer_xlmroberta.decode(element, skip_special_tokens=True)\n",
    "    print(detokenized_sentences_xlmroberta)\n",
    "pr.disable()\n",
    "print(\"Printing stats for decoding unbatched data\")\n",
    "print(\"------------------------------------------\")\n",
    "pr.print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8f4635",
   "metadata": {},
   "source": [
    "Use time module to get the response time of tokenizing batched data using the BERT tokenization model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058fc692",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_encode = time.time()  \n",
    "tokenized_sentences = tokenizer_bert.batch_encode_plus(text_lines, add_special_tokens=True)\n",
    "end_time_encode = time.time()  \n",
    "\n",
    "response_time_encode = end_time_encode - start_time_encode\n",
    "print(f\"Response time for encoding batched data with BERT tokenization model: {response_time_encode} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f2e17f",
   "metadata": {},
   "source": [
    "Use time module to get the response time of tokenizing unbatched data using the BERT tokenization model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94dcf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_encode = time.time() \n",
    "for element in text_lines:\n",
    "    encoding_unbatched = tokenizer_bert.encode_plus(element, add_special_tokens=True, truncation=True)\n",
    "end_time_encode = time.time()  \n",
    "\n",
    "response_time_encode = end_time_encode - start_time_encode\n",
    "print(f\"Response time for encoding unbatched data with BERT tokenization model: {response_time_encode} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a99af8",
   "metadata": {},
   "source": [
    "Use time module to get the response time of tokenizing batched data using the openAI tokenization model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c28ce50",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_encode = time.time() \n",
    "tokenized_sentences_openai = tokenizer_openai.batch_encode_plus(text_lines, add_special_tokens=True)\n",
    "end_time_encode = time.time()\n",
    "\n",
    "response_time_encode = end_time_encode - start_time_encode\n",
    "print(f\"Response time for encoding batched data with openAI tokenization model: {response_time_encode} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76962ca0",
   "metadata": {},
   "source": [
    "Use time module to get the response time of tokenizing unbatched data using the openAI tokenization model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566ec020",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_encode = time.time() \n",
    "for element in text_lines:\n",
    "    encoding_unbatched = tokenizer_openai.encode_plus(element, add_special_tokens=True, truncation=True)\n",
    "    #print(encoding_unbatched)\n",
    "end_time_encode = time.time()\n",
    "\n",
    "response_time_encode = end_time_encode - start_time_encode\n",
    "print(f\"Response time for encoding unbatched data with openAI tokenization model: {response_time_encode} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64756b2c",
   "metadata": {},
   "source": [
    "Use time module to get the response time of tokenizing batched data using the XLMRoberta tokenization model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15696275",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_encode = time.time() \n",
    "tokenized_sentences_xlmroberta = tokenizer_xlmroberta.batch_encode_plus(text_lines, add_special_tokens=True)\n",
    "end_time_encode = time.time()\n",
    "\n",
    "response_time_encode = end_time_encode - start_time_encode\n",
    "print(f\"Response time for encoding batched data with XLMRoberta tokenization model: {response_time_encode} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbffc619",
   "metadata": {},
   "source": [
    "Use time module to get the response time of tokenizing unbatched data using the XLMRoberta tokenization model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b778994",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_encode = time.time() \n",
    "for element in text_lines:\n",
    "    encoding_unbatched = tokenizer_xlmroberta.encode_plus(element, add_special_tokens=True, truncation=True)\n",
    "    #print(encoding_unbatched)\n",
    "end_time_encode = time.time()\n",
    "\n",
    "response_time_encode = end_time_encode - start_time_encode\n",
    "print(f\"Response time for encoding unbatched data with XLMRoberta tokenization model: {response_time_encode} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc09aa5a",
   "metadata": {},
   "source": [
    "Use time module to get the response time of detokenizing batched data using the BERT tokenization model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b8ae27",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_encode = time.time() \n",
    "detokenized_sentences_bert = tokenizer_bert.batch_decode(ids_bert)\n",
    "end_time_encode = time.time()\n",
    "\n",
    "#print(detokenized_sentences_bert)\n",
    "\n",
    "response_time_encode = end_time_encode - start_time_encode\n",
    "print(f\"Response time for detokenizing batched data with BERT tokenization model: {response_time_encode} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445eb0ce",
   "metadata": {},
   "source": [
    "Use time module to get the response time of detokenizing unbatched data using the BERT tokenization model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d8176a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_encode = time.time()\n",
    "for element in ids_bert:\n",
    "    detokenized_sentence_bert = tokenizer_bert.decode(element, skip_special_tokens=True)\n",
    "    #print(detokenized_sentence_bert)\n",
    "end_time_encode = time.time()\n",
    "\n",
    "response_time_encode = end_time_encode - start_time_encode\n",
    "print(f\"Response time for detokenizing unbatched data with BERT tokenization model: {response_time_encode} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a29ff6",
   "metadata": {},
   "source": [
    "Use time module to get the response time of detokenizing batched data using the openAI tokenization model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b412f35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_encode = time.time()\n",
    "detokenized_sentences_openai = tokenizer_openai.batch_decode(ids_openai)\n",
    "end_time_encode = time.time()\n",
    "\n",
    "#print(detokenized_sentences_openai)\n",
    "\n",
    "response_time_encode = end_time_encode - start_time_encode\n",
    "print(f\"Response time for detokenizing batched data with openAI tokenization model: {response_time_encode} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abff2c8",
   "metadata": {},
   "source": [
    "Use time module to get the response time of detokenizing unbatched data using the openAI tokenization model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a68999",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_encode = time.time()\n",
    "for element in ids_openai:\n",
    "    detokenized_sentence_openai = tokenizer_openai.decode(element, skip_special_tokens=True)\n",
    "    #print(detokenized_sentence_openai)\n",
    "end_time_encode = time.time()\n",
    "\n",
    "response_time_encode = end_time_encode - start_time_encode\n",
    "print(f\"Response time for detokenizing unbatched data with openAI tokenization model: {response_time_encode} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad79be91",
   "metadata": {},
   "source": [
    "Use time module to get the response time of detokenizing batched data using the XLMRoberta tokenization model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0923a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_encode = time.time()\n",
    "detokenized_sentences_xlmroberta = tokenizer_xlmroberta.batch_decode(ids_xlmroberta)\n",
    "end_time_encode = time.time()\n",
    "\n",
    "response_time_encode = end_time_encode - start_time_encode\n",
    "print(f\"Response time for detokenizing unbatched data with openAI tokenization model: {response_time_encode} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c95eb8",
   "metadata": {},
   "source": [
    "Use time module to get the response time of detokenizing unbatched data using the XLMRoberta tokenization model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0ed1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_encode = time.time()\n",
    "for element in ids_xlmroberta:\n",
    "    detokenized_sentences_xlmroberta = tokenizer_xlmroberta.decode(element, skip_special_tokens=True)\n",
    "    #print(detokenized_sentences_xlmroberta)\n",
    "end_time_encode = time.time()\n",
    "\n",
    "response_time_encode = end_time_encode - start_time_encode\n",
    "print(f\"Response time for detokenizing unbatched data with openAI tokenization model: {response_time_encode} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
